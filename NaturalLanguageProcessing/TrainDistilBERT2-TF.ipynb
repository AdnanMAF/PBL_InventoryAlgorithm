{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f911d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup, environment, and initial imports\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_KERAS_NO_ATOMIC_CHECKPOINT'] = '1'\n",
    "\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from typing import List, Dict\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    TFDistilBertForSequenceClassification,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be12daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1650, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "# Enable mixed precision on TensorFlow if GPU supports it\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b95842",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TFTrainingConfig:\n",
    "    \"\"\"Configuration for TensorFlow DistilBERT training.\"\"\"\n",
    "    labeled_file: str = '../Data/NLP/news_dataset_id2_labeled.csv'\n",
    "    max_sequence_length: int = 128\n",
    "    test_size: float = 0.25\n",
    "    val_split: float = 0.3\n",
    "    model_name: str = 'distilbert-base-uncased'\n",
    "    num_labels: int = 3\n",
    "    num_epochs: int = 8\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 3e-5\n",
    "    early_stopping_patience: int = 3\n",
    "    augmentation_factor: int = 3\n",
    "    output_dir: str = './tf_models'\n",
    "    model_save_path: str = './tf_models/distilbert_final'\n",
    "    history_save_path: str = './tf_models/training_history.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6e4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Keras implementation of Focal Loss.\n",
    "    α = 1, γ = 2 by default.\n",
    "    Expects logits input (from_logits=True).\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, from_logits=True, name='focal_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        if self.from_logits:\n",
    "            ce = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=tf.cast(y_true, tf.int32),\n",
    "                logits=y_pred\n",
    "            )\n",
    "            pt = tf.exp(-ce)\n",
    "        else:\n",
    "            y_true_ohe = tf.one_hot(tf.cast(y_true, tf.int32), depth=y_pred.shape[-1])\n",
    "            ce = -tf.reduce_sum(y_true_ohe * tf.math.log(y_pred + 1e-9), axis=-1)\n",
    "            pt = tf.reduce_sum(y_true_ohe * y_pred, axis=-1)\n",
    "\n",
    "        focal_factor = self.alpha * tf.pow(1 - pt, self.gamma)\n",
    "        loss = focal_factor * ce\n",
    "        return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2758a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmenter:\n",
    "    \"\"\"Simple text augmentation for minority classes.\"\"\"\n",
    "    @staticmethod\n",
    "    def synonym_replacement(text: str, n: int = 2) -> str:\n",
    "        words = text.split()\n",
    "        if len(words) < 3:\n",
    "            return text\n",
    "        indices = random.sample(range(len(words)), min(n, len(words)//3))\n",
    "        for i in indices:\n",
    "            if i < len(words) - 1:\n",
    "                words[i], words[i+1] = words[i+1], words[i]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_text(text: str, method: str = 'synonym') -> str:\n",
    "        if method == 'synonym':\n",
    "            return DataAugmenter.synonym_replacement(text)\n",
    "        else:\n",
    "            return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc979a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines 'title', 'description', and first 150 tokens of 'content' into 'training_text'.\n",
    "    \"\"\"\n",
    "    def _combine_fields(row):\n",
    "        parts = []\n",
    "        for field in ['title', 'description', 'content']:\n",
    "            if field in row and pd.notna(row[field]):\n",
    "                text = str(row[field]).strip()\n",
    "                if field == 'content':\n",
    "                    text = ' '.join(text.split()[:150])\n",
    "                if text:\n",
    "                    parts.append(text)\n",
    "        return ' '.join(parts)\n",
    "\n",
    "    df['training_text'] = df.apply(_combine_fields, axis=1)\n",
    "    df = df[df['training_text'].str.len() > 10].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa1f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:28:04,797 - INFO - Raw dataset shape: (675, 8)\n"
     ]
    }
   ],
   "source": [
    "# Initialize configuration and constants\n",
    "config = TFTrainingConfig()\n",
    "LABEL_MAPPING = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "# Load and prepare data\n",
    "raw_df = pd.read_csv(config.labeled_file)\n",
    "logger.info(f\"Raw dataset shape: {raw_df.shape}\")\n",
    "\n",
    "df = prepare_training_text(raw_df)\n",
    "df['label'] = df['stock_sentiment'].map(LABEL_MAPPING)\n",
    "df = df.dropna(subset=['label']).reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48fcce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:28:04,840 - INFO - Class distribution (pre-augmentation):\n",
      "2025-06-03 10:28:04,841 - INFO -   Neutral: 466  (69.0%)\n",
      "2025-06-03 10:28:04,841 - INFO -   Positive: 188  (27.9%)\n",
      "2025-06-03 10:28:04,842 - INFO -   Negative: 21  (3.1%)\n",
      "2025-06-03 10:28:04,844 - INFO - Augmenting 0 (21 → 63)\n",
      "2025-06-03 10:28:04,856 - INFO - Augmenting 2 (188 → 233)\n",
      "2025-06-03 10:28:04,872 - INFO - Dataset shape after augmentation: (762, 10)\n"
     ]
    }
   ],
   "source": [
    "# Log class distribution\n",
    "dist = df['stock_sentiment'].value_counts()\n",
    "logger.info(\"Class distribution (pre-augmentation):\")\n",
    "for lab, count in dist.items():\n",
    "    logger.info(f\"  {lab}: {count}  ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Augment minority classes\n",
    "class_counts = df['label'].value_counts()\n",
    "max_count = class_counts.max()\n",
    "augmented_rows = []\n",
    "\n",
    "for label in [0, 2]:  # Negative and Positive\n",
    "    class_df = df[df['label'] == label]\n",
    "    current = len(class_df)\n",
    "    target = min(max_count // 2, current * config.augmentation_factor)\n",
    "    if target > current:\n",
    "        augment_needed = target - current\n",
    "        logger.info(f\"Augmenting {label} ({current} → {target})\")\n",
    "        for _ in range(augment_needed):\n",
    "            row = class_df.sample(1).iloc[0].copy()\n",
    "            row['training_text'] = DataAugmenter.augment_text(row['training_text'])\n",
    "            augmented_rows.append(row)\n",
    "\n",
    "if augmented_rows:\n",
    "    aug_df = pd.DataFrame(augmented_rows)\n",
    "    df = pd.concat([df, aug_df], ignore_index=True)\n",
    "    logger.info(f\"Dataset shape after augmentation: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c18a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:28:04,886 - INFO - Final split → Train: 571, Val: 133, Test: 58\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "texts = df['training_text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=config.test_size, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=config.val_split, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "logger.info(f\"Final split → Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "019ee0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:28:05,297 - INFO - Tokenizing splits...\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and tokenize data\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "def batch_tokenize(texts: List[str]) -> Dict[str, np.ndarray]:\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding='longest',\n",
    "        max_length=config.max_sequence_length,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': enc['input_ids'],\n",
    "        'attention_mask': enc['attention_mask']\n",
    "    }\n",
    "\n",
    "logger.info(\"Tokenizing splits...\")\n",
    "train_enc = batch_tokenize(train_texts)\n",
    "val_enc = batch_tokenize(val_texts)\n",
    "test_enc = batch_tokenize(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801bb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "def make_tf_dataset(encodings: Dict[str, np.ndarray], labels: List[int], batch_size: int, shuffle: bool = False):\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "    labels_arr = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(({\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }, labels_arr))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(labels_arr), seed=42)\n",
    "\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_tf_dataset(train_enc, train_labels, config.batch_size, shuffle=True)\n",
    "val_ds = make_tf_dataset(val_enc, val_labels, config.batch_size, shuffle=False)\n",
    "test_ds = make_tf_dataset(test_enc, test_labels, config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ddcf35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    config.model_name,\n",
    "    num_labels=config.num_labels,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "loss_fn = FocalLoss(alpha=1.0, gamma=2.0, from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63d7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output directories\n",
    "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(config.model_save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b33c61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to track best validation accuracy\n",
    "class BestModelTracker(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_weights = None\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_acc = logs.get('val_accuracy', 0)\n",
    "        if current_val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = current_val_acc\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.patience_counter = 0\n",
    "            logger.info(f\"New best validation accuracy: {current_val_acc:.4f}\")\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "        if self.patience_counter >= config.early_stopping_patience:\n",
    "            logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=config.early_stopping_patience,\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_tracker = BestModelTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89c0b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:28:09,850 - INFO - Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.5797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:29:32,696 - INFO - New best validation accuracy: 0.6316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 83s 2s/step - loss: 0.3786 - accuracy: 0.5797 - val_loss: 0.3291 - val_accuracy: 0.6316\n",
      "Epoch 2/8\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.6515"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:30:42,935 - INFO - New best validation accuracy: 0.7218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 70s 2s/step - loss: 0.2935 - accuracy: 0.6515 - val_loss: 0.2579 - val_accuracy: 0.7218\n",
      "Epoch 3/8\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.8021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:31:53,296 - INFO - New best validation accuracy: 0.8045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 70s 2s/step - loss: 0.1739 - accuracy: 0.8021 - val_loss: 0.2001 - val_accuracy: 0.8045\n",
      "Epoch 4/8\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:33:03,362 - INFO - New best validation accuracy: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 70s 2s/step - loss: 0.0731 - accuracy: 0.9194 - val_loss: 0.1796 - val_accuracy: 0.8421\n",
      "Epoch 5/8\n",
      "36/36 [==============================] - 70s 2s/step - loss: 0.0261 - accuracy: 0.9807 - val_loss: 0.2443 - val_accuracy: 0.8120\n",
      "Epoch 6/8\n",
      "36/36 [==============================] - 70s 2s/step - loss: 0.0136 - accuracy: 0.9825 - val_loss: 0.2842 - val_accuracy: 0.7895\n",
      "Epoch 7/8\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9947Restoring model weights from the end of the best epoch: 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:32,856 - INFO - Early stopping triggered after 7 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 70s 2s/step - loss: 0.0074 - accuracy: 0.9947 - val_loss: 0.3107 - val_accuracy: 0.8346\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "logger.info(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=config.num_epochs,\n",
    "    callbacks=[early_stop, best_tracker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce40a31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:32,942 - INFO - Restored best model weights (val_acc: 0.8421)\n"
     ]
    }
   ],
   "source": [
    "# Restore best weights if available\n",
    "if best_tracker.best_weights is not None:\n",
    "    model.set_weights(best_tracker.best_weights)\n",
    "    logger.info(f\"Restored best model weights (val_acc: {best_tracker.best_val_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff88a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open(config.history_save_path, 'w') as f:\n",
    "    json.dump(history.history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f54012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:32,977 - INFO - Evaluating on test set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 534ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:36,463 - INFO - Test Accuracy: 0.7759\n",
      "2025-06-03 10:36:36,464 - INFO - Test F1 (macro): 0.8024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         5\n",
      "     Neutral       0.78      0.89      0.83        35\n",
      "    Positive       0.69      0.50      0.58        18\n",
      "\n",
      "    accuracy                           0.78        58\n",
      "   macro avg       0.82      0.80      0.80        58\n",
      "weighted avg       0.77      0.78      0.77        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "logger.info(\"Evaluating on test set...\")\n",
    "predictions = model.predict(test_ds).logits\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
    "\n",
    "test_acc = accuracy_score(true_labels, pred_labels)\n",
    "test_f1_macro = f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "logger.info(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "logger.info(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "label_names = ['Negative', 'Neutral', 'Positive']\n",
    "report = classification_report(true_labels, pred_labels, target_names=label_names, output_dict=True)\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb3d6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:37,147 - INFO - ✅ Hugging Face format saved successfully: ./tf_models/distilbert_final\\hf_pretrained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476774AE20>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:38,430 - WARNING - Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476774AE20>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E87C100>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:38,446 - WARNING - Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E87C100>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E883700>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:38,467 - WARNING - Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E883700>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E88DD00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:38,490 - WARNING - Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E88DD00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E89D340>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:38,511 - WARNING - Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002476E89D340>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002470C245940>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:38,531 - WARNING - Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000002470C245940>, because it is not built.\n",
      "2025-06-03 10:36:49,205 - WARNING - Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.\n",
      "2025-06-03 10:36:56,256 - WARNING - ⚠️ Failed to save TensorFlow SavedModel due to file locking: {{function_node __wrapped__SaveV2_dtypes_324_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to rename: ./tf_models/distilbert_final\\saved_model\\variables\\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate486516577388143674 to: ./tf_models/distilbert_final\\saved_model\\variables\\variables_temp/part-00000-of-00001.data-00000-of-00001 : The process cannot access the file because it is being used by another process.\n",
      "; Broken pipe [Op:SaveV2]\n",
      "2025-06-03 10:36:56,861 - INFO - ✅ Model weights saved as alternative: ./tf_models/distilbert_final\\weights\\model_weights.h5\n",
      "2025-06-03 10:36:56,862 - INFO - ✅ Model config saved: ./tf_models/distilbert_final\\weights\\model_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save final model and tokenizer\n",
    "hf_export_dir = os.path.join(config.model_save_path, \"hf_pretrained\")\n",
    "saved_model_dir = os.path.join(config.model_save_path, \"saved_model\")\n",
    "weights_dir = os.path.join(config.model_save_path, \"weights\")\n",
    "\n",
    "Path(hf_export_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(saved_model_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(weights_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save in Hugging Face format (this usually works)\n",
    "try:\n",
    "    model.save_pretrained(hf_export_dir)\n",
    "    tokenizer.save_pretrained(hf_export_dir)\n",
    "    logger.info(f\"✅ Hugging Face format saved successfully: {hf_export_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Failed to save Hugging Face format: {e}\")\n",
    "\n",
    "# Try to save as TensorFlow SavedModel\n",
    "try:\n",
    "    model.save(saved_model_dir, save_format=\"tf\")\n",
    "    logger.info(f\"✅ TensorFlow SavedModel saved successfully: {saved_model_dir}\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"⚠️ Failed to save TensorFlow SavedModel due to file locking: {e}\")\n",
    "    \n",
    "    # Alternative: Save just the weights\n",
    "    try:\n",
    "        weights_path = os.path.join(weights_dir, \"model_weights.h5\")\n",
    "        model.save_weights(weights_path)\n",
    "        logger.info(f\"✅ Model weights saved as alternative: {weights_path}\")\n",
    "        \n",
    "        # Save model config for reconstruction\n",
    "        config_path = os.path.join(weights_dir, \"model_config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'model_name': config.model_name,\n",
    "                'num_labels': config.num_labels,\n",
    "                'max_sequence_length': config.max_sequence_length\n",
    "            }, f, indent=2)\n",
    "        logger.info(f\"✅ Model config saved: {config_path}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        logger.error(f\"❌ Failed to save weights as well: {e2}\")\n",
    "        logger.info(\"💡 You can still use the Hugging Face format if it saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f744603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:36:57,028 - INFO - 🎯 Training complete! Final results:\n",
      "2025-06-03 10:36:57,029 - INFO -    📊 Test Accuracy: 0.7759\n",
      "2025-06-03 10:36:57,030 - INFO -    📊 Test F1 (macro): 0.8024\n",
      "2025-06-03 10:36:57,031 - INFO -    💾 Main model directory: ./tf_models/distilbert_final\n",
      "2025-06-03 10:36:57,032 - INFO -    🤗 Hugging Face format: ./tf_models/distilbert_final\\hf_pretrained\n",
      "2025-06-03 10:36:57,033 - INFO -    🔧 TensorFlow SavedModel: ./tf_models/distilbert_final\\saved_model\n",
      "2025-06-03 10:36:57,035 - INFO -    ⚖️ Model weights backup: ./tf_models/distilbert_final\\weights\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results\n",
    "with open(os.path.join(config.model_save_path, \"test_results.json\"), 'w') as f:\n",
    "    json.dump({\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_f1_macro': float(test_f1_macro),\n",
    "        'classification_report': report\n",
    "    }, f, indent=2)\n",
    "\n",
    "logger.info(f\"🎯 Training complete! Final results:\")\n",
    "logger.info(f\"   📊 Test Accuracy: {test_acc:.4f}\")\n",
    "logger.info(f\"   📊 Test F1 (macro): {test_f1_macro:.4f}\")\n",
    "logger.info(f\"   💾 Main model directory: {config.model_save_path}\")\n",
    "if os.path.exists(hf_export_dir):\n",
    "    logger.info(f\"   🤗 Hugging Face format: {hf_export_dir}\")\n",
    "if os.path.exists(saved_model_dir) and len(os.listdir(saved_model_dir)) > 0:\n",
    "    logger.info(f\"   🔧 TensorFlow SavedModel: {saved_model_dir}\")\n",
    "if os.path.exists(weights_dir) and len(os.listdir(weights_dir)) > 0:\n",
    "    logger.info(f\"   ⚖️ Model weights backup: {weights_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
